{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from gensim.models import Word2Vec\n",
    "import dateutil\n",
    "from scipy.sparse import lil_matrix # To build sparse feature matrices, if you like\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Using the Steam category data, build training/test sets consisting of 10,000 reviews each. Code to do so is provided in the stub. We'll start by building features to represent the common words. Start by removing punctuation and capitalization, and finding the 1,000 most common words across all reviews ('text' field) in the training set. See the 'text mining' lectures for code for this process. Report the 10 most common words, along with their frequencies, as a list of (frequncy, word) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"steam_category.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data in training and test set\n",
    "Ntrain = 10000\n",
    "Ntest = 10000\n",
    "\n",
    "dataTrain = dataset[:Ntrain]\n",
    "dataTest = dataset[Ntrain:Ntrain + Ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize dict of word:appearance_count\n",
    "wordCount = defaultdict(int)\n",
    "\n",
    "#set of punctuation to reference\n",
    "sp = set(string.punctuation)\n",
    "\n",
    "for d in dataTrain: #for review in dataset\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in sp]) #forms a list of all words (remove capitalization) that are not punctuation\n",
    "    ws = r.split() #splits that list into words\n",
    "    for w in ws: #traverses through words & adds a count to dict. for each appearance\n",
    "        wordCount[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of dict. entries as can't .sort() dict. type -> sort from most to least common\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort(reverse = True)\n",
    "\n",
    "#grab list of words where x is in top 100 spots of sorted list, this will be a b.o.w feature vector for models\n",
    "common_words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34211, 'the'),\n",
       " (19392, 'and'),\n",
       " (18791, 'a'),\n",
       " (18077, 'to'),\n",
       " (15043, 'game'),\n",
       " (14095, 'of'),\n",
       " (13000, 'is'),\n",
       " (12735, 'you'),\n",
       " (12204, 'i'),\n",
       " (11824, 'it')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList([x[0] for x in answers['Q1']], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Build bag-of-words feature vectors by counting the instances of these 1,000 words in each review. Set the labels (y) to be the 'genreID' column for the training instances. You may use these labels directly with sklearn's LogisticRegression model, which will automatically perform multiclass classification. Report performance (accuracy) on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordId = dict(zip(common_words, range(len(common_words))))\n",
    "wordSet = set(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building X\n",
    "def feature(datum): #function that creates b.o.w feature\n",
    "    feat = [0]*len(common_words) #initialize feature vector of length 1000, all currently 0 count\n",
    "    review = ''.join([c for c in datum['text'].lower() if not c in sp])\n",
    "    for w in review.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset during loop\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in dataset] #feature vectors for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d['genre'] for d in dataset] #response variable for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset for train/test split\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, ytrain)\n",
    "y_pred = mod.predict(Xtest);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (y_pred == ytest)\n",
    "#accuracy = .6368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6368"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q2'] = sum(correct) / len(correct)\n",
    "answers['Q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What is the inverse document frequency of the words 'character', 'game', 'length', 'a', and 'it'? What are their td-idf scores in the first (training) review (using log base 10, unigrams only, following the first definition of tf-idf given in the slides)? All frequencies etc. should be calculated using the training data only. Your answer should be a list of five (idf, tfidf) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document frequency = # of documents that contain the term t\n",
    "df = defaultdict(int)\n",
    "for d in dataTrain:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in sp])\n",
    "    for w in set(r.split()):\n",
    "        df[w] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#idf - take the log base 10 of (# of documents / # of documents that contain that term)\n",
    "idf = {}\n",
    "for w, freq in df.items():\n",
    "    idf[w] = math.log10(len(dataTrain) / freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first (training) data review\n",
    "first_review = dataTrain[0]\n",
    "\n",
    "tf = defaultdict(int) #initialize term freq. dictionary - how often term t appears in that document\n",
    "r = ''.join([c for c in first_review['text'].lower() if not c in sp])\n",
    "for w in r.split():\n",
    "    tf[w] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to calculate TF-IDF for\n",
    "words_to_calculate = ['character', 'game', 'length', 'a', 'it']\n",
    "tf_idf = {}\n",
    "idf_tfidf_pairs = []\n",
    "\n",
    "for word in words_to_calculate:\n",
    "    tf_value = tf.get(word)  #get the term frequency of the word, default to 0 if not present\n",
    "    idf_value = idf.get(word)  #get the IDF value of the word, default to 0 if not present\n",
    "    tf_idf[word] = tf_value * idf_value  #compute TF-IDF\n",
    "    tfidf_value = tf_value * idf_value \n",
    "    idf_tfidf_pairs.append((idf_value, tfidf_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = idf_tfidf_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.453457336521869, 1.453457336521869),\n",
       " (0.22951619056889208, 0.45903238113778416),\n",
       " (2.2441251443275085, 4.488250288655017),\n",
       " (0.3047810810948491, 2.4382486487587927),\n",
       " (0.376647318462008, 1.129941955386024)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList([x[0] for x in answers['Q3']], 5)\n",
    "assertFloatList([x[1] for x in answers['Q3']], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Adapt your unigram model to use the tdidf scores of words, rather than a bag-of-words representation. That is, rather than your features containing the word counts for the 1000 most common unigrams. Report the accuracy of this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 1000 most common words from document frequency\n",
    "top_unigrams = sorted(df.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "vocab = [word for word, freq in top_unigrams]  # The 1000 most common words\n",
    "X = []\n",
    "for d in dataset:\n",
    "    #compute term freq for that document\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in d['text'].lower() if c not in sp])\n",
    "    for w in r.split():\n",
    "        tf[w] += 1\n",
    "\n",
    "    #compute tf-idf for the document (use idf that we already calculated in Q3)\n",
    "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
    "    \n",
    "    #create a vector using the top 1000 unigrams\n",
    "    vector = [tfidf.get(word, 0) for word in vocab]\n",
    "    X.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d['genre'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, ytrain)\n",
    "y_pred = mod.predict(Xtest)\n",
    "correct = (y_pred == ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6082"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q4'] = sum(correct) / len(correct)\n",
    "answers['Q4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Which review in the test set has the highest cosine similarity compared to the first review in the training set, in terms of their tf-idf representation (considering unigrams only). Provide the cosine similarity score and the reviewID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0 \n",
    "    for a1, a2 in zip(x1, x2):\n",
    "        numer += a1 * a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1 * norm2)\n",
    "    return 0\n",
    "\n",
    "tfidf_train_first = Xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreating test dataset with reviewID for reference\n",
    "Xtest = []  # This will contain tuples of (reviewID, tfidf_vector)\n",
    "\n",
    "for d in dataTest:\n",
    "    reviewID = d['reviewID']  #xxtract the reviewID\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in d['text'].lower() if c not in sp])\n",
    "    for w in r.split():\n",
    "        tf[w] += 1\n",
    "    \n",
    "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
    "\n",
    "    vector = [tfidf.get(word, 0) for word in vocab]\n",
    "\n",
    "    Xtest.append((reviewID, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for review in Xtest:\n",
    "    similarity = Cosine(review[1], tfidf_train_first)\n",
    "    similarities.append((similarity, reviewID))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reviewID, tfidf_test in enumerate(Xtest):  #assuming Xtest contains test set tfidf vectors\n",
    "    similarity = Cosine(tfidf_train_first, tfidf_test)  #compute cosine similarity\n",
    "    similarities.append((reviewID, int(similarity)))  #store reviewID and similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.sort(key = lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5016951620600787, 'r84358104')"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q5'] = similarities[0]\n",
    "answers['Q5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q5'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Try to improve upon the performance of the above classifiers from questions 2 and 4 by using different dictionary sizes, or changing the regularization constant C passed to the logistic regression model. Report the performance of your solution.\n",
    "\n",
    "Use the first half (10,000) of the corpus for training and the rest for testing (code to read the data is provided in the stub). Process review without capitalization or punctuation (and without using stemming or stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Accuracy: 0.6439\n",
      "C: 0.2, Accuracy: 0.6446\n",
      "C: 0.3, Accuracy: 0.644\n",
      "C: 0.4, Accuracy: 0.6409\n",
      "C: 0.5, Accuracy: 0.6395\n",
      "C: 0.6, Accuracy: 0.6381\n",
      "C: 0.7, Accuracy: 0.6366\n",
      "C: 0.8, Accuracy: 0.6348\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[318], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#accuracy = .6368\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#C: 0.1, Accuracy: 0.6439\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m results\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "#Model in question 2\n",
    "wordId = dict(zip(common_words, range(len(common_words))))\n",
    "wordSet = set(common_words)\n",
    "#building X\n",
    "def feature(datum): #function that creates b.o.w feature\n",
    "    feat = [0]*len(common_words) #initialize feature vector of length 1000, all currently 0 count\n",
    "    review = ''.join([c for c in datum['text'].lower() if not c in sp])\n",
    "    for w in review.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset during loop\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in dataset] #feature vectors for entire dataset\n",
    "y = [d['genre'] for d in dataset] #response variable for entire dataset\n",
    "\n",
    "#subset for train/test split\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "C_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "results=[]\n",
    "for C in C_values:\n",
    "    mod = linear_model.LogisticRegression(C=C, max_iter=5000) \n",
    "    mod.fit(Xtrain, ytrain)\n",
    "    \n",
    "\n",
    "    y_pred = mod.predict(Xtest)\n",
    "\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    results.append((C, accuracy))\n",
    "    print(f\"C: {C}, Accuracy: {accuracy}\")\n",
    "#accuracy = .6368\n",
    "#C: 0.2, Accuracy: 0.6446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01, Accuracy: 0.6337\n",
      "C: 0.02, Accuracy: 0.6383\n",
      "C: 0.03, Accuracy: 0.6386\n",
      "C: 0.04, Accuracy: 0.6377\n",
      "C: 0.05, Accuracy: 0.6387\n",
      "C: 0.06, Accuracy: 0.6372\n",
      "C: 0.07, Accuracy: 0.6358\n",
      "C: 0.08, Accuracy: 0.6343\n"
     ]
    }
   ],
   "source": [
    "#Model in question 4\n",
    "top_unigrams = sorted(df.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "vocab = [word for word, freq in top_unigrams]  # The 1000 most common words\n",
    "X = []\n",
    "for d in dataset:\n",
    "    #compute term freq for that document\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in d['text'].lower() if c not in sp])\n",
    "    for w in r.split():\n",
    "        tf[w] += 1\n",
    "\n",
    "    #compute tf-idf for the document (use idf that we already calculated in Q3)\n",
    "    tfidf = {word: tf[word] * idf.get(word, 0) for word in tf}\n",
    "    \n",
    "    #create a vector using the top 1000 unigrams\n",
    "    vector = [tfidf.get(word, 0) for word in vocab]\n",
    "    X.append(vector)\n",
    "y = [d['genre'] for d in dataset]\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "C_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
    "results2 = []\n",
    "\n",
    "for C in C_values:\n",
    "  \n",
    "    mod = linear_model.LogisticRegression(C=C, max_iter=5000) \n",
    "    mod.fit(Xtrain, ytrain)\n",
    "    \n",
    "\n",
    "    y_pred = mod.predict(Xtest)\n",
    "    accuracy = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    results2.append((C, accuracy))\n",
    "    print(f\"C: {C}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelq2 = results[1]\n",
    "modelq4 = results2[4]\n",
    "#used model with better performance below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6446"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q6'] = (modelq2[1])\n",
    "answers['Q6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "This task should be completed using the entire dataset of 20,000 reviews from Goodreads:\n",
    "\n",
    "Using the word2vec library in gensim, fit an item2vec model, treating each sentence as a temporally-ordered list of items per user. Use parameters min_count = 1, size = 5, window = 3, sg = 1. Report the 5 most similar items to the book from the first review along with their similarity scores (your answer can be the output of the similar_by_word function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "reviews = []\n",
    "reviewsPerUser = defaultdict(list)\n",
    "\n",
    "f = gzip.open(\"young_adult_20000.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    d['datetime'] = dateutil.parser.parse(d['date_added'])\n",
    "    reviewsPerUser[d['user_id']].append((d['datetime'], d['book_id']))\n",
    "    reviews.append(d['review_text'])\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'dc3763cdb9b2cae805882878eebb6a32',\n",
       " 'book_id': '18471619',\n",
       " 'review_id': '66b2ba840f9bd36d6d27f46136fe4772',\n",
       " 'rating': 3,\n",
       " 'review_text': 'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.',\n",
       " 'date_added': 'Thu Dec 05 10:44:25 -0800 2013',\n",
       " 'date_updated': 'Thu Dec 05 10:45:15 -0800 2013',\n",
       " 'read_at': 'Tue Nov 05 00:00:00 -0800 2013',\n",
       " 'started_at': '',\n",
       " 'n_votes': 0,\n",
       " 'n_comments': 0,\n",
       " 'datetime': datetime.datetime(2013, 12, 5, 10, 44, 25, tzinfo=tzoffset(None, -28800))}"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewTokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in reviews:\n",
    "    r = ''.join([c for c in r.lower() if not c in sp])\n",
    "    tokens = []\n",
    "    for w in r.split():\n",
    "        tokens.append(w)\n",
    "    reviewTokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sherlock',\n",
       " 'holmes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'vampires',\n",
       " 'of',\n",
       " 'london',\n",
       " 'release',\n",
       " 'date',\n",
       " 'april',\n",
       " '2014',\n",
       " 'publisher',\n",
       " 'darkhorse',\n",
       " 'comics',\n",
       " 'story',\n",
       " 'by',\n",
       " 'sylvain',\n",
       " 'cordurie',\n",
       " 'art',\n",
       " 'by',\n",
       " 'laci',\n",
       " 'colors',\n",
       " 'by',\n",
       " 'axel',\n",
       " 'gonzabo',\n",
       " 'cover',\n",
       " 'by',\n",
       " 'jean',\n",
       " 'sebastien',\n",
       " 'rossbach',\n",
       " 'isdn',\n",
       " '9781616552664',\n",
       " 'msrp',\n",
       " '1799',\n",
       " 'hardcover',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'died',\n",
       " 'fighting',\n",
       " 'professor',\n",
       " 'moriarty',\n",
       " 'in',\n",
       " 'the',\n",
       " 'reichenbach',\n",
       " 'falls',\n",
       " 'at',\n",
       " 'least',\n",
       " 'thats',\n",
       " 'what',\n",
       " 'the',\n",
       " 'press',\n",
       " 'claims',\n",
       " 'however',\n",
       " 'holmes',\n",
       " 'is',\n",
       " 'alive',\n",
       " 'and',\n",
       " 'well',\n",
       " 'and',\n",
       " 'taking',\n",
       " 'advantage',\n",
       " 'of',\n",
       " 'his',\n",
       " 'presumed',\n",
       " 'death',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'the',\n",
       " 'globe',\n",
       " 'unfortunately',\n",
       " 'holmess',\n",
       " 'plans',\n",
       " 'are',\n",
       " 'thwarted',\n",
       " 'when',\n",
       " 'a',\n",
       " 'plague',\n",
       " 'of',\n",
       " 'vampirism',\n",
       " 'haunts',\n",
       " 'britain',\n",
       " 'this',\n",
       " 'book',\n",
       " 'collects',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'vampires',\n",
       " 'of',\n",
       " 'london',\n",
       " 'volumes',\n",
       " '1',\n",
       " 'and',\n",
       " '2',\n",
       " 'originally',\n",
       " 'created',\n",
       " 'by',\n",
       " 'french',\n",
       " 'publisher',\n",
       " 'soleil',\n",
       " 'darkhorse',\n",
       " 'comics',\n",
       " 'when',\n",
       " 'i',\n",
       " 'received',\n",
       " 'this',\n",
       " 'copy',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'vampires',\n",
       " 'of',\n",
       " 'london',\n",
       " 'i',\n",
       " 'was',\n",
       " 'ecstatic',\n",
       " 'the',\n",
       " 'cover',\n",
       " 'art',\n",
       " 'was',\n",
       " 'awesome',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'about',\n",
       " 'two',\n",
       " 'of',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'things',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'and',\n",
       " 'vampires',\n",
       " 'i',\n",
       " 'couldnt',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'dive',\n",
       " 'into',\n",
       " 'this',\n",
       " 'unfortunately',\n",
       " 'that',\n",
       " 'is',\n",
       " 'where',\n",
       " 'my',\n",
       " 'excitement',\n",
       " 'ended',\n",
       " 'the',\n",
       " 'story',\n",
       " 'takes',\n",
       " 'place',\n",
       " 'a',\n",
       " 'month',\n",
       " 'after',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'supposed',\n",
       " 'death',\n",
       " 'in',\n",
       " 'his',\n",
       " 'battle',\n",
       " 'with',\n",
       " 'professor',\n",
       " 'moriarty',\n",
       " 'sherlocks',\n",
       " 'plan',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'hidden',\n",
       " 'and',\n",
       " 'out',\n",
       " 'of',\n",
       " 'site',\n",
       " 'are',\n",
       " 'ruined',\n",
       " 'when',\n",
       " 'on',\n",
       " 'a',\n",
       " 'trip',\n",
       " 'with',\n",
       " 'his',\n",
       " 'brother',\n",
       " 'mycroft',\n",
       " 'they',\n",
       " 'stumble',\n",
       " 'on',\n",
       " 'the',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'vampires',\n",
       " 'that',\n",
       " 'is',\n",
       " 'about',\n",
       " 'as',\n",
       " 'much',\n",
       " 'of',\n",
       " 'sherlocks',\n",
       " 'character',\n",
       " 'that',\n",
       " 'comes',\n",
       " 'through',\n",
       " 'the',\n",
       " 'book',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'even',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'the',\n",
       " 'story',\n",
       " 'really',\n",
       " 'because',\n",
       " 'nothing',\n",
       " 'and',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'nothing',\n",
       " 'stuck',\n",
       " 'with',\n",
       " 'me',\n",
       " 'after',\n",
       " 'reading',\n",
       " 'it',\n",
       " 'i',\n",
       " 'never',\n",
       " 'ever',\n",
       " 'got',\n",
       " 'the',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'this',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'nor',\n",
       " 'any',\n",
       " 'real',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'mystery',\n",
       " 'or',\n",
       " 'crime',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'sherlock',\n",
       " 'somehow',\n",
       " 'battling',\n",
       " 'vampires',\n",
       " 'that',\n",
       " 'should',\n",
       " 'have',\n",
       " 'had',\n",
       " 'absolutely',\n",
       " 'no',\n",
       " 'trouble',\n",
       " 'snuffing',\n",
       " 'him',\n",
       " 'out',\n",
       " 'in',\n",
       " 'a',\n",
       " 'fight',\n",
       " 'but',\n",
       " 'somehow',\n",
       " 'always',\n",
       " 'surviving',\n",
       " 'and',\n",
       " 'holding',\n",
       " 'his',\n",
       " 'own',\n",
       " 'against',\n",
       " 'supernatural',\n",
       " 'super',\n",
       " 'powerful',\n",
       " 'blazingly',\n",
       " 'fast',\n",
       " 'creatures',\n",
       " 'the',\n",
       " 'cover',\n",
       " 'art',\n",
       " 'is',\n",
       " 'awesome',\n",
       " 'and',\n",
       " 'it',\n",
       " 'truly',\n",
       " 'made',\n",
       " 'me',\n",
       " 'excited',\n",
       " 'to',\n",
       " 'read',\n",
       " 'this',\n",
       " 'but',\n",
       " 'everything',\n",
       " 'else',\n",
       " 'feel',\n",
       " 'completely',\n",
       " 'flat',\n",
       " 'for',\n",
       " 'me',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'telling',\n",
       " 'myself',\n",
       " 'that',\n",
       " 'its',\n",
       " 'a',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'translate',\n",
       " 'mystery',\n",
       " 'details',\n",
       " 'emotion',\n",
       " 'but',\n",
       " 'then',\n",
       " 'i',\n",
       " 'remembered',\n",
       " 'reading',\n",
       " 'dc',\n",
       " 'comics',\n",
       " 'identity',\n",
       " 'crisis',\n",
       " 'and',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'was',\n",
       " 'a',\n",
       " 'load',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'i',\n",
       " 'know',\n",
       " 'its',\n",
       " 'unfair',\n",
       " 'to',\n",
       " 'compare',\n",
       " 'the',\n",
       " 'two',\n",
       " 'as',\n",
       " 'identity',\n",
       " 'crisis',\n",
       " 'had',\n",
       " 'popular',\n",
       " 'mystery',\n",
       " 'author',\n",
       " 'brad',\n",
       " 'meltzer',\n",
       " 'writing',\n",
       " 'it',\n",
       " 'right',\n",
       " 'yeahno',\n",
       " 'the',\n",
       " 'standard',\n",
       " 'was',\n",
       " 'set',\n",
       " 'that',\n",
       " 'day',\n",
       " 'and',\n",
       " 'there',\n",
       " 'is',\n",
       " 'more',\n",
       " 'than',\n",
       " 'enough',\n",
       " 'talent',\n",
       " 'out',\n",
       " 'there',\n",
       " 'to',\n",
       " 'create',\n",
       " 'a',\n",
       " 'great',\n",
       " 'story',\n",
       " 'in',\n",
       " 'a',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'that',\n",
       " 'being',\n",
       " 'said',\n",
       " 'it',\n",
       " 'wasnt',\n",
       " 'a',\n",
       " 'horrible',\n",
       " 'story',\n",
       " 'it',\n",
       " 'just',\n",
       " 'didnt',\n",
       " 'grip',\n",
       " 'me',\n",
       " 'for',\n",
       " 'feel',\n",
       " 'anything',\n",
       " 'like',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'to',\n",
       " 'me',\n",
       " 'it',\n",
       " 'was',\n",
       " 'easy',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'but',\n",
       " 'i',\n",
       " 'felt',\n",
       " 'no',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'tension',\n",
       " 'stakes',\n",
       " 'or',\n",
       " 'compassion',\n",
       " 'for',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'as',\n",
       " 'far',\n",
       " 'as',\n",
       " 'the',\n",
       " 'vampires',\n",
       " 'go',\n",
       " 'its',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'expect',\n",
       " 'anymore',\n",
       " 'as',\n",
       " 'there',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'these',\n",
       " 'days',\n",
       " 'this',\n",
       " 'was',\n",
       " 'the',\n",
       " 'more',\n",
       " 'classic',\n",
       " 'version',\n",
       " 'which',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'prefer',\n",
       " 'but',\n",
       " 'again',\n",
       " 'i',\n",
       " 'didnt',\n",
       " 'find',\n",
       " 'anything',\n",
       " 'that',\n",
       " 'portrayed',\n",
       " 'their',\n",
       " 'dominance',\n",
       " 'calm',\n",
       " 'confidence',\n",
       " 'or',\n",
       " 'sexuality',\n",
       " 'there',\n",
       " 'was',\n",
       " 'definitely',\n",
       " 'a',\n",
       " 'presence',\n",
       " 'of',\n",
       " 'their',\n",
       " 'physical',\n",
       " 'prowess',\n",
       " 'but',\n",
       " 'somehow',\n",
       " 'that',\n",
       " 'was',\n",
       " 'lost',\n",
       " 'on',\n",
       " 'me',\n",
       " 'as',\n",
       " 'easily',\n",
       " 'as',\n",
       " 'sherlock',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'defend',\n",
       " 'himself',\n",
       " 'i',\n",
       " 'know',\n",
       " 'it',\n",
       " 'wouldnt',\n",
       " 'do',\n",
       " 'to',\n",
       " 'kill',\n",
       " 'of',\n",
       " 'the',\n",
       " 'main',\n",
       " 'character',\n",
       " 'but',\n",
       " 'this',\n",
       " 'would',\n",
       " 'have',\n",
       " 'a',\n",
       " 'been',\n",
       " 'a',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'build',\n",
       " 'around',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'and',\n",
       " 'beguiling',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'a',\n",
       " 'vampire',\n",
       " 'that',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'so',\n",
       " 'many',\n",
       " 'years',\n",
       " 'of',\n",
       " 'experience',\n",
       " 'another',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'showcase',\n",
       " 'sherlocks',\n",
       " 'intellect',\n",
       " 'in',\n",
       " 'a',\n",
       " 'battle',\n",
       " 'of',\n",
       " 'wits',\n",
       " 'over',\n",
       " 'strength',\n",
       " 'in',\n",
       " 'something',\n",
       " 'more',\n",
       " 'suitable',\n",
       " 'for',\n",
       " 'this',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'story',\n",
       " 'as',\n",
       " 'apposed',\n",
       " 'to',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'make',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'an',\n",
       " 'action',\n",
       " 'movie',\n",
       " 'maybe',\n",
       " 'i',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'much',\n",
       " 'and',\n",
       " 'hoped',\n",
       " 'to',\n",
       " 'have',\n",
       " 'at',\n",
       " 'least',\n",
       " 'a',\n",
       " 'gripping',\n",
       " 'premise',\n",
       " 'or',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'interesting',\n",
       " 'plot',\n",
       " 'or',\n",
       " 'mystery',\n",
       " 'but',\n",
       " 'i',\n",
       " 'didnt',\n",
       " 'find',\n",
       " 'it',\n",
       " 'here',\n",
       " 'this',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'must',\n",
       " 'have',\n",
       " 'for',\n",
       " 'serious',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'fans',\n",
       " 'that',\n",
       " 'have',\n",
       " 'to',\n",
       " 'collect',\n",
       " 'everything',\n",
       " 'about',\n",
       " 'him',\n",
       " 'but',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'great',\n",
       " 'story',\n",
       " 'inside',\n",
       " 'a',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " 'i',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'pass',\n",
       " 'on',\n",
       " 'this',\n",
       " 'one',\n",
       " 'that',\n",
       " 'artwork',\n",
       " 'is',\n",
       " 'good',\n",
       " 'cover',\n",
       " 'is',\n",
       " 'great',\n",
       " 'story',\n",
       " 'is',\n",
       " 'lacking',\n",
       " 'so',\n",
       " 'i',\n",
       " 'am',\n",
       " 'giving',\n",
       " 'it',\n",
       " '25',\n",
       " 'out',\n",
       " 'of',\n",
       " '5',\n",
       " 'stars']"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewTokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Word2Vec(reviewTokens,\n",
    "                  min_count=1, # Words/items with fewer instances are discarded\n",
    "                  vector_size=5, # Model dimensionality\n",
    "                  window=3, # Window size\n",
    "                  sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrows', 0.9992873072624207),\n",
       " ('spiderwoman', 0.9991419911384583),\n",
       " ('priest', 0.9988325238227844),\n",
       " ('affair', 0.9988138675689697),\n",
       " ('sketch', 0.9987421631813049),\n",
       " ('benefits', 0.998714029788971),\n",
       " ('potter', 0.9986669421195984),\n",
       " ('principals', 0.998612642288208),\n",
       " ('inferno', 0.9984824061393738),\n",
       " ('trio', 0.9984773993492126)]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.wv.similar_by_word('sherlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item2vec implementation\n",
    "reviewLists = []\n",
    "for u in reviewsPerUser:\n",
    "    rl = list(reviewsPerUser[u])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Word2Vec(reviewLists,\n",
    "                  min_count=1, # Words/items with fewer instances are discarded\n",
    "                  vector_size=5, # Model dimensionality\n",
    "                  window=3, # Window size\n",
    "                  sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model6.wv.similar_by_word('18471619')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('841237', 0.9972667694091797),\n",
       " ('34658929', 0.9909661412239075),\n",
       " ('10555316', 0.9896025061607361),\n",
       " ('13449407', 0.9894213080406189),\n",
       " ('16002011', 0.9874613285064697)]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q7'] = res[:5]\n",
    "answers['Q7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList([x[1] for x in answers['Q7']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw4.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
